{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9225da21",
   "metadata": {},
   "source": [
    "# Login and data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990072f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aicrowd-cli\n",
    "API_KEY = \"\"\n",
    "!aicrowd login --api-key $API_KEY\n",
    "\n",
    "!mkdir data\n",
    "!aicrowd dataset download --challenge research-paper-classification -j 3 -o data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef877a",
   "metadata": {},
   "source": [
    "# Utility functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(value=23):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed_all(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202669f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_preprocessing(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in data:\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(text),\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True)\n",
    "        \n",
    "        input_ids.append(encoded_text.get(\"input_ids\"))\n",
    "        attention_masks.append(encoded_text.get(\"attention_mask\"))\n",
    "    \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f22dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 200\n",
    "CLIP = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75c40e",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51745f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0dd13",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"./data/train.csv\")\n",
    "validation_dataset = pd.read_csv(\"./data/val.csv\")[1:]\n",
    "test_data = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dataset.text.values\n",
    "y_train = train_dataset.label.values\n",
    "X_val = validation_dataset.text.values\n",
    "y_val = validation_dataset.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce590f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate([X_train, X_val])\n",
    "encoded_data = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_masks = bert_preprocessing(X_train)\n",
    "val_inputs, val_masks = bert_preprocessing(X_val)\n",
    "\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6bb27",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8656615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        in_features, hid_dim, out_features = 768, 16, 4\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hid_dim, out_features)\n",
    "        )\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        last_hid_state_cls = outputs[0][:, 0, :]\n",
    "        \n",
    "        logits = self.classifier(last_hid_state_cls)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "def initialize_model(epochs=4):\n",
    "    \n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "    bert_classifier.to(device)\n",
    "    \n",
    "    optimizer = AdamW(bert_classifier.parameters(), lr=5e-5, eps=1e-8)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b622d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1f71d",
   "metadata": {},
   "source": [
    "# Loop functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9742327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    t0_epoch, t0_batch = time.time(), time.time()\n",
    "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            \n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                \n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "                \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        if evaluation == True:\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), \"research_paper_bert.pt\")\n",
    "                \n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b23bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloder):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            \n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        \n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "    \n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    \n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_dataloder):\n",
    "    model.eval()\n",
    "    \n",
    "    all_logits = []\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "        \n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    \n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbfc1c",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5525ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
    "full_train_sampler = RandomSampler(full_train_data)\n",
    "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=batch_size)\n",
    "\n",
    "set_seed(23)\n",
    "\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, epochs=10, evaluation=True)\n",
    "evaluate(bert_classifier, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80551caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier.load_state_dict(torch.load(\"research_paper_bert.pt\"))\n",
    "loss, accuracy = evaluate(bert_classifier, val_dataloader)\n",
    "print(f\"Best model loss: {loss}\")\n",
    "print(f\"Best model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3377825",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4cfd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, test_masks = bert_preprocessing(test_data.text)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict(bert_classifier, test_dataloader)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "test_data[\"label\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd0305",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!mkdir assets\n",
    "test_data.to_csv(os.path.join(\"assets\", \"submission.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff679c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aicrowd notebook submit -c research-paper-classification -a assets --no-verify"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
