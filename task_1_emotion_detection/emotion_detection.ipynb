{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84340bc1",
   "metadata": {},
   "source": [
    "# Login and data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aicrowd-cli\n",
    "API_KEY = \"\"\n",
    "!aicrowd login --api-key $API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!aicrowd dataset download --challenge emotion-detection -j 3 -o data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ece14",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(value=23):\n",
    "    random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed_all(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_preprocessing(data: list):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in data:\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(text),\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True)\n",
    "    \n",
    "        input_ids.append(encoded_text.get(\"input_ids\"))\n",
    "        attention_masks.append(encoded_text.get(\"attention_mask\"))\n",
    "    \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65942bc3",
   "metadata": {},
   "source": [
    "# Import and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3af8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 250\n",
    "CLIP = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b5324",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"./data/train.csv\")\n",
    "val_dataset = pd.read_csv(\"./data/val.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dataset.text.values\n",
    "y_train = train_dataset.label.values\n",
    "X_val = validation_dataset.text.values\n",
    "y_val = validation_dataset.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef72ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate([X_train, X_val])\n",
    "encoded_data = [tokenizer.encode(text, add_special_tokens=True) for text in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_masks = bert_preprocessing(X_train)\n",
    "val_inputs, val_masks = bert_preprocessing(X_val)\n",
    "\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b5b97",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        in_features, hid_dim, out_features = 768, 8, 2\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, hid_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hid_dim, out_features)\n",
    "        )\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def initialize_model(epochs=4):\n",
    "        bert_classifier = BertClassifier(freeze_bert=False)\n",
    "        bert_classifier.to(DEVICE)\n",
    "        \n",
    "        optimizer = AdamW(bert_classifier.parameters(), lr=5e-5, eps=1e-8)\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "        \n",
    "        return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88365eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ee278",
   "metadata": {},
   "source": [
    "# Loop functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    t0_epoch, t0_batch = time.time(), time.time()\n",
    "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            \n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                \n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "                \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        if evaluation == True:\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), \"sentiment_classification_bert.pt\")\n",
    "                \n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloder):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            \n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        \n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "    \n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    \n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_dataloder):\n",
    "    model.eval()\n",
    "    \n",
    "    all_logits = []\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "        \n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    \n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae88a3",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
    "full_train_sampler = RandomSampler(full_train_data)\n",
    "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=batch_size)\n",
    "\n",
    "set_seed(23)\n",
    "\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, epochs=5, evaluation=True)\n",
    "evaluate(bert_classifier, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier.load_state_dict(torch.load(\"sentiment_classification_bert.pt\"))\n",
    "loss, accuracy = evaluate(bert_classifier, val_dataloader)\n",
    "print(f\"Best model loss: {loss}\")\n",
    "print(f\"Best model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb79fb0",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, test_masks = bert_preprocessing(test_data.text)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict(bert_classifier, test_dataloader)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "test_data[\"label\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ae499",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!mkdir assets\n",
    "test_data.to_csv(os.path.join(\"assets\", \"submission.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aicrowd notebook submit -c emotion-detection -a assets --no-verify"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
